Created as a diploma of Yandex Practicum / DevOps training course.
------------------------------------------------------------------

# Momo Store aka Пельменная №2 (Дипломная работа 03.2024)
 
<img width="900" alt="image" src="https://user-images.githubusercontent.com/9394918/167876466-2c530828-d658-4efe-9064-825626cc6db5.png">

Исходные тексты магазина доступны в настоящем репозитории.
Состав:
 - backend (папка)
   -- выполнен на языке Go
 - frontend (папка)
   -- выполнен на Vue.js, компиляция происходит средствами npm v16.

Установка магазина возможна в двух вариантах:
 - отдельный ПК (linux)
 - кластер kubernetes

## Установка на выделенный ПК
Данная возможность реализована в ознакомительных целях и для целей тестирования.

Требование к ПК:
 - x64 Linux (deban-based)
 - docker
 
Для установки на ПК необходимо задать в настройках CI/CD переменные (они уже присутствуют и имеют ознакомительные значения):
 - DEPLOYMENT_HOST - fqdn ПК назначения
 - DEPLOYMENT_HOST - имя пользователя, под которым будет осуществляться установка
 - SSH_PRIVATE_KEY - private-key ssh ключа, под которым будет осуществляться доступ на ПК назначения. Параметр в base64 кодировке.
 - SSH_KNOWN_HOSTS - список открытых ssh-ключей целевого ПК. Сбор и получение значения может быть выполнен посредством запуска:
  ```bash
  ssh-keyscan fqdn #fqdn ПК назначения
  ```
Установка приложения осуществляется запуском на выполнения задач пайплайна <i>deploy-frontend-locally</i> и <i>deploy-backend-locally</i> <b>вручную</b>. Задачи становятся доступны после запуска пайплайна сборки на выполнение (при изменении кода приложения или состава папок/файлов /frontend/ | /backend/). По завершению установки, на целевом ПК появляютсяи стартуют два контейнера docker. Приложение становится доступным при обращении из внешней сети по порту http://fqdn:8080 (fqdn ПК назначения).


```bash
#пример
$ docker ps
CONTAINER ID   IMAGE                                                                                      COMMAND                  CREATED         STATUS                            PORTS                                               NAMES
1f9b17737300   gitlab.praktikum-services.ru:5050/std-ext-001-027/diploma/dumplings-frontend:1.0.1166131   "/docker-entrypoint.…"   4 minutes ago   Up 3 minutes (healthy)            80/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   frontend
f400231f6d27   gitlab.praktikum-services.ru:5050/std-ext-001-027/diploma/dumplings-backend:1.0.1166132    "./api"                  8 minutes ago   Up 7 minutes (healthy)            8081/tcp                                            backend
```

## Установка в кластере Kubernetes
Установка может производиться в готовый, имеющийся кластер (см. ниже), или же подготовленный сценарием (<b>deploy.sh</b>), представленными в этом репозитории.
целиком репозиторий клонировать нет необходимости, достаточно скопировать скрипт. Необходимые действия будут произведены автоматически.
Небольшая видеопрезентация на указанную тему доступна по данной ссылке https://gitlab.praktikum-services.ru/std-ext-001-027/diploma/-/blob/master/chrome_94Vx2UQjIZ.mp4

<b>Важно!</b> Перед запуском сценария, необходимо отредактировать заголовок файла сценария, в котором содержатся необходимые инициализационные переменные. Сценарий, в данном контексте сборки, нацелен на работу в облаке Yandex Cloud. Сценарий полностью автоматизирован, однако, необходимо задать несколько ключевых параметров:

 - YC_ORG_ID - это идентфикатор рабочего пространства (организации), в которой будет развернуто приложение
 - YC_FOLDER_ID - это идентификатор папки, которая должна быть предварительно создана для установки приложения
 - YC_SERVICE_ACCOUNT - наименование административного аккаунта в разворачиваемом кластере

 - FRONTEND_EXTERNAL_FQDN - URL будущего приложения для доступа к нему из-вне. Фактически представляет собой NS-зону в DNS, в     
   которой будет доступно приложение после установки в кластер. Важный параметр развертывания. Вы должны заранее создать или стать владельцем данной DNS-зоны. <b>Важно! Параметр указывается без префикса www.</b>
 - TLS_STORE_SECRET_NAME - необязательный параметр, указывающий на наименование k8s secret name для сохранения данных сертификата
   используеются как точка взаимосвязи между менеджером сертификатов и ingress-контроллером приложения
 - PUBLIC_OBJECT_STORAGE_NAME - наименование public-storage, публичного хранилища (с открытым доступом), куда будет загружена 
   папка /frontend/img/ - статические изображения сайта
 - PRIVATE_TF_STATE_OBJECT_STORAGE_NAME - наименование private-storage. Будет использоваться лишь в случае использования  
   Terraform для сохранения state файла текущего развертывания.
 - REPOSITORY - путь к репозиторию. По сути является путем к текущему репозиторию, где расположен исходный код. Используется 
   скриптом для загрузки данных текущей версии приложения и/или инфраструктурных файлов конфигурации.
 - REPOSITORY_IMAGES_URL - ссылка получения образов. Данный параметр необходим для получения доступа к образам сборки приложения
   Используется для дальнейшей доставки их в kubernetes.

Сценарий запускается без параметров (в общем случае). Результатом работы, будет внешний IP адрес приложения, развернутого в кластере. Данная информация указывается с целью дальнейших действий по регистрации данного адреса в DNS-зоне <i>FRONTEND_EXTERNAL_FQDN</i>.

При запуска в стандартном варианте (без параметров) сценарий будет загружать всё необходимое для выполнения. Однако, на ПК, где производится запуск, должен быть предварительно установлен bash, jq, envsubst, getopt, curl.

Процесс установки приложения выглядит следующим образом: первый шаг - запрос учетных данных от репозитория, где содержится программный код (параметр REPOSITORY). Далее, после загрузки Yandex CLI, Terraform, Helm, Kubernetes CLI. RClone будет произведена инцициализация yandex-окружения, и произведен запрос учетных данных yandex id, id облака/организации. Все эти данные будут переданны Terraform, которая воссоздаст ждя Вас:
- кластер k8s с включенным логгированием
- учетные данные для управления кластером
- объектное хранилище для сохранения данных работы terraform
- объектное хранилище для хранения статических изображений frontend-сайта
По завершению работы terraform, будет сформирован kubercli конфигурационный файл и установлено приложение в созданный кластер. Затем, в этот же кластер последовательно будет установлен ingress контроллер с балансировщиком и менеджер сертификатов. Приложение будет работать с сертификатами Let's encrypt. Выдача необходимого сертификата будет произведена автоматически.
По завершению процедуры, будет произведена синхронизация папки frontend/img с созданным в кластере облачном хранилище. Ссылки на новое храналище, соответственно, передаются через переменные окружения при deploy приложения.
В конце работы сценария, будет сформирован статический (без привязки к yandex-cli) kubeconfig файл, и выдан на экран. Для удобства за ним, следом, будет представлен base64 вариант файла. Эти данные можно использовать для доступа к кластеру из-вне, а также для работы с pipeline в текущем репозитории. Полученное base64 значение необходимо указать в ci/cd переменную <b>KUBECONFIG_DATA</b>. Это обеспечит доступ раннера к созданному кластеру.

Сценарий можно запускать несколько раз (в случае ошибок, например), однако, второй и последующие запуски будут завершаться с ошибкой из-за присутствия склонированной ветки репозитория уже в текущем каталоге. Чтобы избежать подобного развития событий, можно использовать ключ запуска deploy.sh --skip_clone. Этот параметр пропустит шаг по клонированию сборки и перейдет к следующим шагам.

При изменении любого содержимого в чарте dumplings-store-chart, pipline автоматически попытается запусить (но, не запустит) процедуру сборки чарта и установки в кластер. Данная процедура предлагается на запуск вручную, после модерации возможных изменений.

Для успешного выполнения шага деплоя в кластер необходимо предварительно задать переменные в ci/cd:
- BACKEND_IMAGE_VERSION - см.выше
- FRONTEND_IMAGE_VERSION - см.выше
- FRONTEND_EXTERNAL_FQDN - см.выше
- TLS_STORE_SECRET_NAME - см.выше
- REPOSITORY_IMAGES_URL - см.выше
- PUBLIC_OBJECT_STORAGE_NAME - см. выше
- NEXUS_ACCESS_URL - поскольку, сборка приложения, а также разработка чартов helm использует nexus для хранения версионированных артефактов (результатов) сборки, этот параметр задает url для доступа в nexus-хранилище.  
- NEXUS_PASSWORD - учетные данные в nexus-хранилище.
- NEXUS_USERNAME - учетные данные в nexus-хранилище.
- NEXUS_REPOSITORY_NAME_HELM - url-репозитория в nexus хранилище, для хранения чартов helm.
- KUBECONFIG_DATA - base64 вариант kubeconfig файла для доступа к кластеру

### Установка в готовый кластер.
Необходимо наличие соответствующих привилегий. Установка производится через запуск сценария deploy.sh с параметром --no_infrastructure, представленный в этом репозитории. 
```bash
bash ./deploy.sh --no_infrastructure

#Необходимо задать соответствующие учетные данные. Значением REQUIRED отмечены необходимые/обязательные для задания параметры
helm upgrade --install "dumplings-store" "./diploma/infrastructure/kubernetes/dumplings-store-chart" \
             --set "backend.image.Tag=REQUIRED" \
             --set "frontend.image.Tag=REQUIRED" \
             --set "global.repositoryUrl=REQUIRED" \
             --set "global.k8sHostname=REQUIRED" \
             --set "global.tslStoreSecretName=REQUIRED" \
             --set "backend.environmentConfig[0].name=PUBLIC_OBJECT_STORAGE_URL" \
             --set "backend.environmentConfig[0].value=https://storage.yandexcloud.net/REQUIRED/" \
             --wait \
             --timeout 300s \
             --atomic
```

По завершению процедуры (запуска сценария), будет создано приложение и оно будет доступно по внешнему IP адресу. Однако, доступность приложения по протоколу https будет осуществлена позднее (до двух часов), поскольку подтверждение от cert-manager и проверка Let's Encrypt поступит позже. Следить за данным процессом можно этой компандой. Статус выпуска (Ready) должен быть true.
```bash
$ kubectl get cert
NAME                         READY   SECRET                       AGE
dumplings-store-tls-secret   True    dumplings-store-tls-secret   58m
```

### Разработка и дальнейшее сопровождение приложения.
При разработке и дальнейшем сопровождении данного приложения следует четко придерживаться порядка версионирования. Ci/cd будет автоматически присваивать при каждой сборке новый id сборки. Соответственно, при изменении любой части кода, в папках /frontend /backend запустится автоматически конвейер сборки, тестирования, однако, это не приведет к его автоматической установке или загрузке в хранилище Nexus. Это выполнено специально для проведения модерации необходимости внесения изменений в образ и примененения его в работе. В качестве настоятельных рекоментаций, предлагается отказ от выполнения изменения в ветке master настоящего репозитория и использование отдельной ветки разработки с последующим управляемым процессом слияния отдельной ветки с основной частью проекта.
Для выполнения и поддержки сохранения истории версионирования реализовано сохранение образов сборки в nexus хранилище, для чего, в ci/cd необходимо задать параметры:
- NEXUS_ACCESS_URL - поскольку, сборка приложения, а также разработка чартов helm использует nexus для хранения версионированных артефактов (результатов) сборки, этот параметр задает url для доступа в nexus-хранилище.  
- NEXUS_REPOSITORY_NAME_DOCKER_BACKEND - url-репозитория в nexus хранилище, для хранения raw артефактов (образов контейнеров).
- NEXUS_REPOSITORY_NAME_DOCKER_FRONTEND - url-репозитория в nexus хранилище, для хранения raw артефактов (образов контейнеров).
- NEXUS_PASSWORD - учетные данные в nexus-хранилище.
- NEXUS_USERNAME - учетные данные в nexus-хранилище.

### Мониторинг работы приложения.
Приложение поддерживает мониторинг и логгирование. Для целей получения метрик может быть использовано любое внешнее приложение, например Grafana, Prometheus. В данном репозитории включено все необходимое для мониторинга приложения:
После инсталляции в кластер, в этот же кластер будут установлены Prometheus и Grafana. Необходимо лишь создание двух доменных записей к домену FRONTEND_EXTERNAL_FQDN, а именно grafana.xxx и prometheus.... Доступ осуществляется по протоколу http. Необходимый базовый дашбоард Grafana представлен в данном репозитории.
